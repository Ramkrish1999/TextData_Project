{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56d2f8a2-08de-447e-a768-424794da006b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-04T12:38:11.520057Z",
     "iopub.status.busy": "2025-08-04T12:38:11.518954Z",
     "iopub.status.idle": "2025-08-04T12:40:04.339287Z",
     "shell.execute_reply": "2025-08-04T12:40:04.337698Z",
     "shell.execute_reply.started": "2025-08-04T12:38:11.520057Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~upyterlab (D:\\ Anaconda folder\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~upyterlab (D:\\ Anaconda folder\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~upyterlab (D:\\ Anaconda folder\\Lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-api-python-client in d:\\ anaconda folder\\lib\\site-packages (2.177.0)\n",
      "Requirement already satisfied: textblob in d:\\ anaconda folder\\lib\\site-packages (0.19.0)\n",
      "Requirement already satisfied: nltk in d:\\ anaconda folder\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in d:\\ anaconda folder\\lib\\site-packages (from google-api-python-client) (0.22.0)\n",
      "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0,>=1.32.0 in d:\\ anaconda folder\\lib\\site-packages (from google-api-python-client) (2.40.3)\n",
      "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in d:\\ anaconda folder\\lib\\site-packages (from google-api-python-client) (0.2.0)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5 in d:\\ anaconda folder\\lib\\site-packages (from google-api-python-client) (2.25.1)\n",
      "Requirement already satisfied: uritemplate<5,>=3.0.1 in d:\\ anaconda folder\\lib\\site-packages (from google-api-python-client) (4.2.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in d:\\ anaconda folder\\lib\\site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (1.70.0)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.19.5 in d:\\ anaconda folder\\lib\\site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (4.25.3)\n",
      "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in d:\\ anaconda folder\\lib\\site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (1.26.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.18.0 in d:\\ anaconda folder\\lib\\site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (2.32.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in d:\\ anaconda folder\\lib\\site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=1.32.0->google-api-python-client) (5.3.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in d:\\ anaconda folder\\lib\\site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=1.32.0->google-api-python-client) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in d:\\ anaconda folder\\lib\\site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=1.32.0->google-api-python-client) (4.9.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in d:\\ anaconda folder\\lib\\site-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client) (3.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\ anaconda folder\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\ anaconda folder\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\ anaconda folder\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\ anaconda folder\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (2025.7.14)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in d:\\ anaconda folder\\lib\\site-packages (from rsa<5,>=3.1.4->google-auth!=2.24.0,!=2.25.0,<3.0.0,>=1.32.0->google-api-python-client) (0.4.8)\n",
      "Requirement already satisfied: click in d:\\ anaconda folder\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in d:\\ anaconda folder\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in d:\\ anaconda folder\\lib\\site-packages (from nltk) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in d:\\ anaconda folder\\lib\\site-packages (from nltk) (4.66.5)\n",
      "Requirement already satisfied: colorama in d:\\ anaconda folder\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\RAMAKRISHNA\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\RAMAKRISHNA\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\RAMAKRISHNA\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching comments...\n",
      "Total comments fetched: 5000\n",
      "Preprocessing text and analyzing sentiment...\n",
      "Final shape: (5000, 10)\n",
      "Saved as youtube_comments_sentiment.pkl ‚úÖ\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>comment_id</th>\n",
       "      <th>author</th>\n",
       "      <th>comment</th>\n",
       "      <th>like_count</th>\n",
       "      <th>published_at</th>\n",
       "      <th>updated_at</th>\n",
       "      <th>reply_count</th>\n",
       "      <th>clean_comment</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Z4hVGCWH1Kc</td>\n",
       "      <td>Ugw_Rsdge_gVn71T1uZ4AaABAg</td>\n",
       "      <td>@MrBeast</td>\n",
       "      <td>Go to https://www.teamwater.org to donate!</td>\n",
       "      <td>29400</td>\n",
       "      <td>2025-08-01T16:02:00Z</td>\n",
       "      <td>2025-08-01T16:02:00Z</td>\n",
       "      <td>560</td>\n",
       "      <td>go httpswwwteamwaterorg donate</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Z4hVGCWH1Kc</td>\n",
       "      <td>UgzpYUhNfLQywSicX-h4AaABAg</td>\n",
       "      <td>@marvleman</td>\n",
       "      <td>Wee ar always with u</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-08-04T12:37:27Z</td>\n",
       "      <td>2025-08-04T12:37:27Z</td>\n",
       "      <td>0</td>\n",
       "      <td>wee ar always u</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Z4hVGCWH1Kc</td>\n",
       "      <td>UgwRpyR7deCiusgHpnZ4AaABAg</td>\n",
       "      <td>@Svsilva34</td>\n",
       "      <td>ü´∂üî•</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-08-04T12:36:36Z</td>\n",
       "      <td>2025-08-04T12:36:36Z</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Z4hVGCWH1Kc</td>\n",
       "      <td>UgwRe_f0CQeuLuznIeF4AaABAg</td>\n",
       "      <td>@EducationEntertainment-vm3ho</td>\n",
       "      <td>Namaste sir can I ask a question ‚ùì I from Nepal</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-08-04T12:34:58Z</td>\n",
       "      <td>2025-08-04T12:34:58Z</td>\n",
       "      <td>0</td>\n",
       "      <td>namaste sir ask question nepal</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Z4hVGCWH1Kc</td>\n",
       "      <td>UgxMs45zcpDGEBq19Yh4AaABAg</td>\n",
       "      <td>@JohnKombi-x6d</td>\n",
       "      <td>Cam to DRC üá®üá©üá®üá©üá®üá©üá®üá©üá®üá©üá®üá©üá®üá©üá®üá©üá®üá©üá®üá©üá®üá©üá®üá©üá®üá©üá®üá©</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-08-04T12:34:52Z</td>\n",
       "      <td>2025-08-04T12:34:52Z</td>\n",
       "      <td>0</td>\n",
       "      <td>cam drc</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      video_id                  comment_id                         author  \\\n",
       "0  Z4hVGCWH1Kc  Ugw_Rsdge_gVn71T1uZ4AaABAg                       @MrBeast   \n",
       "1  Z4hVGCWH1Kc  UgzpYUhNfLQywSicX-h4AaABAg                     @marvleman   \n",
       "2  Z4hVGCWH1Kc  UgwRpyR7deCiusgHpnZ4AaABAg                     @Svsilva34   \n",
       "3  Z4hVGCWH1Kc  UgwRe_f0CQeuLuznIeF4AaABAg  @EducationEntertainment-vm3ho   \n",
       "4  Z4hVGCWH1Kc  UgxMs45zcpDGEBq19Yh4AaABAg                 @JohnKombi-x6d   \n",
       "\n",
       "                                           comment  like_count  \\\n",
       "0       Go to https://www.teamwater.org to donate!       29400   \n",
       "1                             Wee ar always with u           0   \n",
       "2                                               ü´∂üî•           0   \n",
       "3  Namaste sir can I ask a question ‚ùì I from Nepal           1   \n",
       "4          Cam to DRC üá®üá©üá®üá©üá®üá©üá®üá©üá®üá©üá®üá©üá®üá©üá®üá©üá®üá©üá®üá©üá®üá©üá®üá©üá®üá©üá®üá©           0   \n",
       "\n",
       "           published_at            updated_at  reply_count  \\\n",
       "0  2025-08-01T16:02:00Z  2025-08-01T16:02:00Z          560   \n",
       "1  2025-08-04T12:37:27Z  2025-08-04T12:37:27Z            0   \n",
       "2  2025-08-04T12:36:36Z  2025-08-04T12:36:36Z            0   \n",
       "3  2025-08-04T12:34:58Z  2025-08-04T12:34:58Z            0   \n",
       "4  2025-08-04T12:34:52Z  2025-08-04T12:34:52Z            0   \n",
       "\n",
       "                    clean_comment sentiment  \n",
       "0  go httpswwwteamwaterorg donate   neutral  \n",
       "1                 wee ar always u   neutral  \n",
       "2                                   neutral  \n",
       "3  namaste sir ask question nepal   neutral  \n",
       "4                         cam drc   neutral  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# üì¶ STEP 1: Install required libraries\n",
    "!pip install google-api-python-client textblob nltk\n",
    "\n",
    "# üì• STEP 2: Import packages\n",
    "import pandas as pd\n",
    "import time\n",
    "import re\n",
    "import nltk\n",
    "import pickle\n",
    "from textblob import TextBlob\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from googleapiclient.discovery import build\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "# üß† STEP 3: Download NLTK data\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# üîê STEP 4: YouTube API Setup\n",
    "api_key = \"AIzaSyCXwVJPuDmFBhnZR1jp2JR8hlgjUPCoH0o\"\n",
    "youtube = build('youtube', 'v3', developerKey=api_key)\n",
    "\n",
    "# üí¨ STEP 5: Comment Fetching Function\n",
    "def get_comments(video_id, limit=5000, pause=1):\n",
    "    comments_data = []\n",
    "    request = youtube.commentThreads().list(\n",
    "        part=\"snippet\",\n",
    "        videoId=video_id,\n",
    "        maxResults=100,\n",
    "        textFormat=\"plainText\"\n",
    "    )\n",
    "    response = request.execute()\n",
    "\n",
    "    while response and len(comments_data) < limit:\n",
    "        for item in response['items']:\n",
    "            snippet = item['snippet']['topLevelComment']['snippet']\n",
    "            comments_data.append({\n",
    "                \"video_id\": video_id,\n",
    "                \"comment_id\": item['id'],\n",
    "                \"author\": snippet.get(\"authorDisplayName\", \"Unknown\"),\n",
    "                \"comment\": snippet.get(\"textDisplay\", \"\"),\n",
    "                \"like_count\": snippet.get(\"likeCount\", 0),\n",
    "                \"published_at\": snippet.get(\"publishedAt\", \"\"),\n",
    "                \"updated_at\": snippet.get(\"updatedAt\", \"\"),\n",
    "                \"reply_count\": item['snippet'].get(\"totalReplyCount\", 0)\n",
    "            })\n",
    "            if len(comments_data) >= limit:\n",
    "                break\n",
    "\n",
    "        if 'nextPageToken' in response and len(comments_data) < limit:\n",
    "            request = youtube.commentThreads().list(\n",
    "                part=\"snippet\",\n",
    "                videoId=video_id,\n",
    "                pageToken=response['nextPageToken'],\n",
    "                maxResults=100,\n",
    "                textFormat=\"plainText\"\n",
    "            )\n",
    "            response = request.execute()\n",
    "            time.sleep(pause)\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    return pd.DataFrame(comments_data)\n",
    "\n",
    "# üßπ STEP 6: Text Preprocessing\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    if pd.isnull(text):\n",
    "        return \"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    words = nltk.word_tokenize(text)\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    return \" \".join(words)\n",
    "\n",
    "# ‚ù§Ô∏è STEP 7: Sentiment Labeling\n",
    "def get_sentiment(text):\n",
    "    if pd.isnull(text) or text.strip() == \"\":\n",
    "        return \"neutral\"\n",
    "    blob = TextBlob(text)\n",
    "    polarity = blob.sentiment.polarity\n",
    "    if polarity > 0:\n",
    "        return \"positive\"\n",
    "    elif polarity < 0:\n",
    "        return \"negative\"\n",
    "    else:\n",
    "        return \"neutral\"\n",
    "\n",
    "# üöÄ STEP 8: Full Pipeline Execution\n",
    "video_id = \"Z4hVGCWH1Kc\"  # MrBeast video\n",
    "print(\"Fetching comments...\")\n",
    "df = get_comments(video_id, limit=5000)\n",
    "print(\"Total comments fetched:\", len(df))\n",
    "\n",
    "print(\"Preprocessing text and analyzing sentiment...\")\n",
    "df[\"clean_comment\"] = df[\"comment\"].apply(preprocess_text)\n",
    "df[\"sentiment\"] = df[\"clean_comment\"].apply(get_sentiment)\n",
    "\n",
    "# Select final 10 columns\n",
    "df_final = df[[\n",
    "    \"video_id\", \"comment_id\", \"author\", \"comment\", \"like_count\",\n",
    "    \"published_at\", \"updated_at\", \"reply_count\", \"clean_comment\", \"sentiment\"\n",
    "]]\n",
    "\n",
    "print(\"Final shape:\", df_final.shape)\n",
    "\n",
    "# üíæ STEP 9: Save using Pickle\n",
    "with open(\"youtube_comments_sentiment.pkl\", \"wb\") as f:\n",
    "    pickle.dump(df_final, f)\n",
    "\n",
    "print(\"Saved as youtube_comments_sentiment.pkl ‚úÖ\")\n",
    "\n",
    "# ‚úÖ STEP 10: Load and verify\n",
    "with open(\"youtube_comments_sentiment.pkl\", \"rb\") as f:\n",
    "    loaded_df = pickle.load(f)\n",
    "\n",
    "loaded_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd5cb2f-d949-42dd-aff1-ad01ea0e14b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
